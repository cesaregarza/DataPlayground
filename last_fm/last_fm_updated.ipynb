{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('newpy38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b845c24f545efb918699511a349d3a292375bcfdcffe586ab011a7a862206127"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from last_fm_secrets import *\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from plotly import express as px\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "import dask_ml\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('last_fm.db')\n",
    "\n",
    "pdix = pd.IndexSlice\n",
    "\n",
    "class Function_Dictionary_Class:\n",
    "    user_agent = \"Halcyon\"\n",
    "    api_key    = api_key\n",
    "    secret     = secret\n",
    "    api_root_url        = \"http://ws.audioscrobbler.com/2.0\"\n",
    "    method              = \"user.getRecentTracks\"\n",
    "    user                = username\n",
    "    json                = \"json\"\n",
    "    root_key            = \"recenttracks\"\n",
    "    attributes          = \"@attr\"\n",
    "    tracks              = \"track\"\n",
    "    mb_id               = \"mbid\"\n",
    "    artist              = \"artist\"\n",
    "    artists             = \"artists\"\n",
    "    name                = \"name\"\n",
    "    album               = \"album\"\n",
    "    date                = \"date\"\n",
    "    uts                 = \"uts\"\n",
    "    text                = \"#text\"\n",
    "    idd                 = \"id\"\n",
    "    release_date        = \"release_date\"\n",
    "    total_tracks        = \"total_tracks\"\n",
    "    duration            = \"duration_ms\"\n",
    "    explicit            = \"explicit\"\n",
    "    popularity          = \"popularity\"\n",
    "    danceability        = \"danceability\"\n",
    "    energy              = \"energy\"\n",
    "    key                 = \"key\"\n",
    "    loudness            = \"loudness\"\n",
    "    mode                = \"mode\"\n",
    "    speechiness         = \"speechiness\"\n",
    "    acousticness        = \"acousticness\"\n",
    "    instrumentalness    = \"instrumentalness\"\n",
    "    liveness            = \"liveness\"\n",
    "    valence             = \"valence\"\n",
    "    tempo               = \"tempo\"\n",
    "    time_signature      = \"time_signature\"\n",
    "    metrics             = [\"danceability\",\n",
    "                           \"energy\", \n",
    "                           \"key\", \n",
    "                           \"loudness\",\n",
    "                           \"mode\",\n",
    "                           \"speechiness\",\n",
    "                           \"instrumentalness\",\n",
    "                           \"acousticness\",\n",
    "                           \"liveness\",\n",
    "                           \"valence\",\n",
    "                           \"tempo\",\n",
    "                           \"time_signature\"]\n",
    "\n",
    "    class DataFrame_Columns_Class:\n",
    "        track_id        = \"track_id\"\n",
    "        artist_id       = \"artist_id\"\n",
    "        album_id        = \"album_id\"\n",
    "        track_name      = \"track_name\"\n",
    "        artist_name     = \"artist_name\"\n",
    "        album_name      = \"album_name\"\n",
    "        date            = \"date\"\n",
    "        album_release   = \"album_release_date\"\n",
    "        album_tracks    = \"album_total_tracks\"\n",
    "        duration        = \"track_duration\"\n",
    "        explicit        = \"explicit\"\n",
    "        popularity      = \"track_popularity\"\n",
    "        track_merge     = \"track_name_merge\"\n",
    "        artist_merge    = \"artist_name_merge\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.df_columns = self.DataFrame_Columns_Class()\n",
    "\n",
    "func_dict = Function_Dictionary_Class()\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"user-agent\": func_dict.user_agent\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.exceptions import SpotifyException\n",
    "\n",
    "auth_manager = SpotifyClientCredentials(client_id=spotify_key, client_secret=spotify_secret)\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "\n",
    "def parse_tracks(json):\n",
    "    new_dict = {\n",
    "        func_dict.df_columns.album_name:    json[func_dict.album][func_dict.name],\n",
    "        func_dict.df_columns.album_id:      json[func_dict.album][func_dict.idd],\n",
    "        func_dict.df_columns.album_release: json[func_dict.album][func_dict.release_date],\n",
    "        func_dict.df_columns.album_tracks:  json[func_dict.album][func_dict.total_tracks],\n",
    "        func_dict.df_columns.artist_name:   json[func_dict.artists][0][func_dict.name],\n",
    "        func_dict.df_columns.artist_id:     json[func_dict.artists][0][func_dict.idd],\n",
    "        func_dict.df_columns.duration:      json[func_dict.duration],\n",
    "        func_dict.df_columns.track_name:    json[func_dict.name],\n",
    "        func_dict.df_columns.track_id:      json[func_dict.idd],\n",
    "        func_dict.df_columns.explicit:      json[func_dict.explicit],\n",
    "        func_dict.df_columns.popularity:    json[func_dict.popularity]\n",
    "    }\n",
    "    return new_dict\n",
    "\n",
    "def parse_features(json):\n",
    "    new_dict = {\n",
    "        func_dict.idd:                          json[func_dict.idd],\n",
    "        func_dict.danceability:                 json[func_dict.danceability],\n",
    "        func_dict.energy:                       json[func_dict.energy],\n",
    "        func_dict.key:                          json[func_dict.key],\n",
    "        func_dict.loudness:                     json[func_dict.loudness],\n",
    "        func_dict.mode:                         json[func_dict.mode],\n",
    "        func_dict.speechiness:                  json[func_dict.speechiness],\n",
    "        func_dict.instrumentalness:             json[func_dict.instrumentalness],\n",
    "        func_dict.acousticness:                 json[func_dict.acousticness],\n",
    "        func_dict.liveness:                     json[func_dict.liveness],\n",
    "        func_dict.valence:                      json[func_dict.valence],\n",
    "        func_dict.tempo:                        json[func_dict.tempo],\n",
    "        func_dict.time_signature:               json[func_dict.time_signature]\n",
    "    }\n",
    "    return new_dict\n",
    "\n",
    "def parse_multiple_features(response_list):\n",
    "    return_list = []\n",
    "    \n",
    "    for track in response_list:\n",
    "        try:\n",
    "            return_list.append(parse_features(track))\n",
    "        except TypeError:\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(return_list)\n",
    "\n",
    "def parse_multiple_tracks(response_json):\n",
    "    list_of_tracks = response_json[func_dict.tracks]\n",
    "    return_list = []\n",
    "\n",
    "    for track in list_of_tracks:\n",
    "        try:\n",
    "            return_list.append(parse_tracks(track))\n",
    "        except TypeError:\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(return_list)\n",
    "\n",
    "def spotify_call(func):\n",
    "    \"\"\"Wrapper for functions that call on the spotify API. Automatically handles SpotifyExceptions\n",
    "\n",
    "    Args:\n",
    "        func (function): Any function to be wrapped\n",
    "    \"\"\"\n",
    "\n",
    "    def wrapped_func(*args, **kwargs):\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "        except SpotifyException as e:\n",
    "            sleep_timer = e.headers[func_dict.retry_after]\n",
    "            #0.1 is an arbitrary choice, can be reduced if necessary\n",
    "            time.sleep(sleep_timer + func_dict.additional_wait)\n",
    "            result = wrapped_func(*args, **kwargs)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return wrapped_func\n",
    "\n",
    "@spotify_call\n",
    "def spotify_next(next_json):\n",
    "    return sp.next(next_json)\n",
    "\n",
    "@spotify_call\n",
    "def spotify_tracks(input_list):\n",
    "    return sp.tracks(input_list)\n",
    "\n",
    "@spotify_call\n",
    "def spotify_audio_features(input_list):\n",
    "    return sp.audio_features(input_list)\n",
    "\n",
    "@spotify_call\n",
    "def retrieve_all_playlists_on_page(playlist_page_df):\n",
    "    \"\"\"Generates a pandas dataframe containing every single track on every single playlist\n",
    "\n",
    "    Args:\n",
    "        playlist_page_df (pd.DataFrame): DataFrame containing the playlist ID and name as columns\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with all the resulting tracks with a genre choice appended\n",
    "    \"\"\"\n",
    "    list_of_dfs = []\n",
    "    for idx, [current_name, current_id] in playlist_page_df.iterrows():\n",
    "        #This is a very rudimentary approach to keep only the genre name\n",
    "        genre_name                          = current_name[len(\"The Sound of \"):]\n",
    "        playlist_df                         = retrieve_all_tracks_from_playlist(current_id)\n",
    "        playlist_df[func_dict.genre_name]   = genre_name\n",
    "        list_of_dfs                        += [playlist_df]\n",
    "    \n",
    "    #Concatenate all the resulting tracks\n",
    "    return pd.concat(list_of_dfs, ignore_index=True)\n",
    "      \n",
    "        \n",
    "def retrieve_all_tracks_from_playlist(playlist_id):\n",
    "    \"\"\"Generates a DataFrame with the track ID for every item in the given playlist\n",
    "\n",
    "    Args:\n",
    "        playlist_id (str): String containing the playlist ID\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the track ID for every track in the playlist\n",
    "    \"\"\"\n",
    "    fields = \"tracks.items.track.id, tracks.next\"\n",
    "    response_json = sp.playlist(playlist_id, fields=fields)[func_dict.tracks]\n",
    "    return_df = pd.json_normalize(response_json[func_dict.items])\n",
    "\n",
    "    while response_json[func_dict.nextt]:\n",
    "        response_json = spotify_next(response_json)\n",
    "        return_df = pd.concat([return_df, pd.json_normalize(response_json[func_dict.items])[[\"track.id\"]]], ignore_index=True)\n",
    "        try:\n",
    "            response_json[func_dict.nextt]\n",
    "        except KeyError:\n",
    "            break\n",
    "    \n",
    "    return return_df\n",
    "\n",
    "\n",
    "def build_training_db():\n",
    "    \"\"\"Iterate through the playlists by user \"thesoundsofspotify\" and retrieve every song with its associated genre\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the track and audio feature information of each track as well as the genre\n",
    "    \"\"\"\n",
    "\n",
    "    #Basic setup\n",
    "    user = \"thesoundsofspotify\"\n",
    "    base_len = len(\"The Sound of \")\n",
    "    response_json = sp.user_playlists(user)\n",
    "    response_df = pd.json_normalize(response_json[func_dict.items])\n",
    "    initial_playlist_no = len(response_df)\n",
    "\n",
    "    #Filter dataframe to only include 'The Sound of'\n",
    "    response_df = response_df.loc[response_df[func_dict.name].apply(len) > base_len, [func_dict.name, func_dict.idd]]\n",
    "\n",
    "    return_df   = retrieve_all_playlists_on_page(response_df)\n",
    "    return_df.to_sql(\"Training_db\", conn, if_exists=\"append\")\n",
    "\n",
    "    total_playlists = response_json['total']\n",
    "    progress        = tqdm(total=total_playlists)\n",
    "    progress.update(initial_playlist_no)\n",
    "\n",
    "    while response_json[func_dict.nextt]:\n",
    "        response_json   = spotify_next(response_json)\n",
    "        response_df     = pd.json_normalize(response_json[func_dict.items])\n",
    "        no_on_page      = len(response_df)\n",
    "        response_df     = response_df.loc[response_df[func_dict.name].apply(len) > base_len, [func_dict.name, func_dict.idd]]\n",
    "        new_df          = retrieve_all_playlists_on_page(response_df)\n",
    "        new_df.to_sql(\"Training_db\", conn, if_exists=\"append\")\n",
    "\n",
    "        return_df       = pd.concat([return_df, new_df])\n",
    "        progress.update(no_on_page)\n",
    "\n",
    "        try:\n",
    "            response_json[func_dict.nextt]\n",
    "        except KeyError:\n",
    "            break\n",
    "    \n",
    "    return return_df\n",
    "\n",
    "\n",
    "def retrieve_tracks_and_features(input_df):\n",
    "    \"\"\"Given an input dataframe with track IDs, retrieve the track information and features\n",
    "\n",
    "    Args:\n",
    "        input_df (pd.DataFrame): DataFrame containing a list of track IDs\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all track information along with audio features\n",
    "    \"\"\"\n",
    "\n",
    "    #Calculate the number of iterations to run for, minus one\n",
    "    iterations = math.ceil(len(input_df) / 50) - 1\n",
    "    #Create an empty list that will contain all the dataframes for each iteration called\n",
    "    dfs = []\n",
    "\n",
    "    for i in tqdm(range(iterations), total=iterations):\n",
    "        #Slice the list of IDs that will be used. This ensures it will only ever be 50 at a time\n",
    "        id_list = input_df[func_dict.df_columns.track_id][50 * i : 50 * (i + 1)]\n",
    "        #retrieve the track JSON for the given list of IDs\n",
    "\n",
    "        track_json = spotify_tracks(id_list)\n",
    "        track_df = parse_multiple_tracks(track_json)\n",
    "\n",
    "        feature_json = spotify_audio_features(id_list)\n",
    "        feature_df = parse_multiple_features(feature_json)\n",
    "\n",
    "        #Merge the two dataframes, using an inner merge to kick out any tracks which would have incomplete information\n",
    "        merged_df = track_df.merge(feature_df, how=\"inner\", left_on=\"track_id\", right_on=\"id\")\n",
    "\n",
    "        dfs.append(merged_df)\n",
    "    \n",
    "    #Repeat the above process but for the final tracks\n",
    "    remaining_tracks    = len(input_df) % 50\n",
    "    final_tracks        = input_df[func_dict.df_columns.track_id][-remaining_tracks:]\n",
    "    return_json         = spotify_tracks(final_tracks)\n",
    "    track_df            = parse_multiple_tracks(return_json)\n",
    "\n",
    "    feature_json        = spotify_audio_features(final_tracks)\n",
    "    feature_df          = parse_multiple_features(feature_json)\n",
    "\n",
    "    merged_df           = track_df.merge(feature_df, how=\"inner\", left_on=\"track_id\", right_on=\"id\")\n",
    "    dfs.append(merged_df)\n",
    "\n",
    "    #Concatenate all the dataframes, ignore the index, and drop duplicates created in the last step\n",
    "    return pd.concat(dfs, ignore_index=True).drop_duplicates(keep=\"first\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "db_path = \"sqlite:///last_fm.db\"\n",
    "client = Client(n_workers = 1, threads_per_worker=16, memory_limit=\"25GB\", processes=False)\n",
    "client\n",
    "feature_ddf = dd.read_sql_table(\"FEATURES\", db_path, \"index\", npartitions=10)\n",
    "genre_track_ddf = dd.read_sql_table(\"TRAINING_DB_2\", db_path, \"index\", npartitions=10)\n",
    "genre_track_ddf = genre_track_ddf.loc[~genre_track_ddf['track_id'].isna()].set_index(\"track_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_feature_ddf = feature_ddf.merge(genre_track_ddf, on=func_dict.df_columns.track_id).set_index(\"track_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import Categorizer, DummyEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    Categorizer(),\n",
    "    DummyEncoder()\n",
    ")\n",
    "pipe.fit(genre_track_ddf)\n",
    "categorized_ddf = pipe.transform(genre_track_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ddf = genre_feature_ddf.merge(categorized_ddf, how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = dask_ml.model_selection.train_test_split(merged_ddf[func_dict.metrics], merged_ddf[categorized_ddf.columns], test_size=0.2, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    layers = [\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(5543, activation=\"sigmoid\")\n",
    "    ]\n",
    "    try:\n",
    "        model = tf.keras.models.Sequential(layers)\n",
    "    except TypeError:\n",
    "        model = build_model()\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "compmodel = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "partial_model = KerasClassifier(build_fn=build_model)\n",
    "model = dask_ml.wrappers.Incremental(partial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "distributed.comm.inproc - WARNING - Closing dangling queue in <InProc  local=inproc://192.168.1.69/30140/1 remote=inproc://192.168.1.69/30140/10>\n",
      "distributed.worker - WARNING - Worker is at 94% memory usage. Pausing worker.  Process memory: 23.56 GB -- Worker memory limit: 25.00 GB\n",
      "distributed.worker - WARNING - Worker is at 53% memory usage. Resuming worker. Process memory: 13.45 GB -- Worker memory limit: 25.00 GB\n",
      "distributed.worker - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 21.67 GB -- Worker memory limit: 25.00 GB\n",
      "distributed.worker - WARNING - Worker is at 57% memory usage. Resuming worker. Process memory: 14.26 GB -- Worker memory limit: 25.00 GB\n",
      "distributed.worker - WARNING -  Compute Failed\n",
      "Function:  _partial_fit\n",
      "args:      (KerasClassifier(\n",
      "\tmodel=None\n",
      "\tbuild_fn=<function build_model at 0x000001B2F1ECA5E0>\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=rmsprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=None\n",
      "\tverbose=1\n",
      "\tcallbacks=None\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=1\n",
      "\tclass_weight=None\n",
      "),                         danceability  energy  key  loudness  mode  \\\n",
      "track_id                                                            \n",
      "2K7aRSKkbLfsGySaCrg9J3         0.456   0.952   10    -6.153     0   \n",
      "2K7htBYvtQF4zl9ca3hBxX         0.633   0.831    9    -5.800     1   \n",
      "2K7l9ExbBvBKXvwNBoyKmf         0.305   0.648    9    -7.350     1   \n",
      "2K7oDusRl1XhFZ6PPM0HOP         0.804   0.653    7    -3.801     1   \n",
      "2K7oDusRl1XhFZ6PPM0HOP         0.804   0.653    7    -3.801     1   \n",
      "...                              ...     ...  ...       ...   ...   \n",
      "2wqvBFhNIjXF3SWNTSuqkQ         0.500   0.235    9   -19.820     0   \n",
      "2wqvEgTvwUCAm4ZpzRUIy4         0.491   0.940    6    -5.908     0   \n",
      "2wqyNwsYDFolK\n",
      "kwargs:    {}\n",
      "Exception: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d768f88d541e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\dask_ml\\wrappers.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_for_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\dask_ml\\wrappers.py\u001b[0m in \u001b[0;36m_fit_for_estimator\u001b[1;34m(self, estimator, X, y, **fit_kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m             result = fit(\n\u001b[0m\u001b[0;32m    480\u001b[0m                 \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\dask_ml\\_partial.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, x, y, compute, shuffle_blocks, random_state, assume_equal_chunks, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\dask\\base.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \"\"\"\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\dask\\base.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\distributed\\client.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, dsk, keys, restrictions, loose_restrictions, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[0;32m   2682\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2683\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2684\u001b[1;33m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2685\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2686\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\distributed\\client.py\u001b[0m in \u001b[0;36mgather\u001b[1;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[0;32m   1991\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1992\u001b[0m                 \u001b[0mlocal_worker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1993\u001b[1;33m             return self.sync(\n\u001b[0m\u001b[0;32m   1994\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gather\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m                 \u001b[0mfutures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\distributed\\client.py\u001b[0m in \u001b[0;36msync\u001b[1;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    837\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 839\u001b[1;33m             return sync(\n\u001b[0m\u001b[0;32m    840\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m             )\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\distributed\\utils.py\u001b[0m in \u001b[0;36msync\u001b[1;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\distributed\\utils.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0merror\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\tornado\\gen.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m                         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\distributed\\client.py\u001b[0m in \u001b[0;36m_gather\u001b[1;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[0;32m   1856\u001b[0m                             \u001b[0mexc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1857\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1858\u001b[1;33m                             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1859\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"skip\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ac\\envs\\newpy38\\lib\\site-packages\\dask_ml\\_partial.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[1;34m(model, x, y, kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_partial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, classes, sample_weight)\u001b[0m\n\u001b[0;32m   1412\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1414\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1415\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    889\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m         \"\"\"\n\u001b[1;32m--> 891\u001b[1;33m         self._fit(\n\u001b[0m\u001b[0;32m    892\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_encoder_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_model_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m         self._fit_keras_model(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scikeras\\wrappers.py\u001b[0m in \u001b[0;36m_check_model_compatibility\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    541\u001b[0m             \u001b[1;31m# we recognize the attribute but do not force it to be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[1;31m# generated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_expected_\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m                 raise ValueError(\n\u001b[0;32m    545\u001b[0m                     \u001b[1;34m\"Detected a Keras model input of size\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                       danceability   energy    key loudness   mode speechiness instrumentalness acousticness liveness  valence    tempo time_signature\n",
       "npartitions=35                                                                                                                                         \n",
       "0001Wtl60puR26ZtSDIF66      float64  float64  int64  float64  int64     float64          float64      float64  float64  float64  float64          int64\n",
       "0NcFUVh63Pyr3vlCd6LAq1          ...      ...    ...      ...    ...         ...              ...          ...      ...      ...      ...            ...\n",
       "...                             ...      ...    ...      ...    ...         ...              ...          ...      ...      ...      ...            ...\n",
       "7bocpX5EAnq0di0gsgzvYA          ...      ...    ...      ...    ...         ...              ...          ...      ...      ...      ...            ...\n",
       "7zzzHZ2sGSdBizrykHrWtd          ...      ...    ...      ...    ...         ...              ...          ...      ...      ...      ...            ...\n",
       "Dask Name: split, 563 tasks"
      ],
      "text/html": "<div><strong>Dask DataFrame Structure:</strong></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>key</th>\n      <th>loudness</th>\n      <th>mode</th>\n      <th>speechiness</th>\n      <th>instrumentalness</th>\n      <th>acousticness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>tempo</th>\n      <th>time_signature</th>\n    </tr>\n    <tr>\n      <th>npartitions=35</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0001Wtl60puR26ZtSDIF66</th>\n      <td>float64</td>\n      <td>float64</td>\n      <td>int64</td>\n      <td>float64</td>\n      <td>int64</td>\n      <td>float64</td>\n      <td>float64</td>\n      <td>float64</td>\n      <td>float64</td>\n      <td>float64</td>\n      <td>float64</td>\n      <td>int64</td>\n    </tr>\n    <tr>\n      <th>0NcFUVh63Pyr3vlCd6LAq1</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7bocpX5EAnq0di0gsgzvYA</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7zzzHZ2sGSdBizrykHrWtd</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<div>Dask Name: split, 563 tasks</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}